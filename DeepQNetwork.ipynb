{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepQNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VccLX2EPXyOC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "import progressbar\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import gym\n",
        "from tensorflow import keras\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import Dense, Embedding, Reshape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enviroment = gym.make(\"Taxi-v3\").env\n",
        "enviroment.render()\n",
        "print('Number of states: {}'.format(enviroment.observation_space.n))\n",
        "print('Number of actions: {}'.format(enviroment.action_space.n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ouXF_JNaEoI",
        "outputId": "f1e93e9e-da62-4179-96c0-520357e9e85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Number of states: 500\n",
            "Number of actions: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, enviroment, optimizer):\n",
        "        \n",
        "        # Initialize atributes\n",
        "        self._state_size = enviroment.observation_space.n\n",
        "        self._action_size = enviroment.action_space.n\n",
        "        self._optimizer = optimizer\n",
        "        \n",
        "        self.expirience_replay = deque(maxlen=2000)\n",
        "        \n",
        "        # Initialize discount and exploration rate\n",
        "        self.gamma = 0.6\n",
        "        self.epsilon = 0.1\n",
        "        \n",
        "        # Build networks\n",
        "        self.q_network = self._build_compile_model()\n",
        "        self.target_network = self._build_compile_model()\n",
        "        self.alighn_target_model()\n",
        "def store(self, state, action, reward, next_state, terminated):\n",
        "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
        "    \n",
        "def _build_compile_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self._state_size, 10, input_length=1))\n",
        "        model.add(Reshape((10,)))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(self._action_size, activation='linear'))\n",
        "        \n",
        "        model.compile(loss='mse', optimizer=self._optimizer)\n",
        "        return model\n",
        "def alighn_target_model(self):\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "    \n",
        "def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return enviroment.action_space.sample()\n",
        "        \n",
        "        q_values = self.q_network.predict(state)\n",
        "        return np.argmax(q_values[0])\n",
        "def retrain(self, batch_size):\n",
        "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
        "        \n",
        "        for state, action, reward, next_state, terminated in minibatch:\n",
        "            \n",
        "            target = self.q_network.predict(state)\n",
        "            \n",
        "            if terminated:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                t = self.target_network.predict(next_state)\n",
        "                target[0][action] = reward + self.gamma * np.amax(t)\n",
        "            \n",
        "            self.q_network.fit(state, target, epochs=1, verbose=0)       "
      ],
      "metadata": {
        "id": "eWLUqvH6YRJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, enviroment, optimizer):      \n",
        "  # Initialize atributes\n",
        "  self._state_size = enviroment.observation_space.n\n",
        "  self._action_size = enviroment.action_space.n\n",
        "  self._optimizer = optimizer\n",
        "\n",
        "  self.expirience_replay = deque(maxlen=2000)\n",
        "\n",
        "  # Initialize discount and exploration rate\n",
        "  self.gamma = 0.6\n",
        "  self.epsilon = 0.1\n",
        "\n",
        "  # Build networks\n",
        "  self.q_network = self._build_compile_model()\n",
        "  self.target_network = self._build_compile_model()\n",
        "  self.alighn_target_model()"
      ],
      "metadata": {
        "id": "sa-Ysc81Zu2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _build_compile_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(self._state_size, 10, input_length=1))\n",
        "    model.add(Reshape((10,)))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(self._action_size, activation='linear'))\n",
        "\n",
        "    model.compile(loss='mse', optimizer=self._optimizer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "CYchCjAWZu_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "        return enviroment.action_space.sample()\n",
        "\n",
        "    q_values = self.q_network.predict(state)\n",
        "    return np.argmax(q_values[0])\n"
      ],
      "metadata": {
        "id": "rdxIs-smZvFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain(self, batch_size):\n",
        "    minibatch = random.sample(self.expirience_replay, batch_size)\n",
        "\n",
        "    for state, action, reward, next_state, terminated in minibatch:\n",
        "\n",
        "        target = self.q_network.predict(state)\n",
        "\n",
        "        if terminated:\n",
        "            target[0][action] = reward\n",
        "        else:\n",
        "            t = self.target_network.predict(next_state)\n",
        "            target[0][action] = reward + self.gamma * np.amax(t)\n",
        "\n",
        "        self.q_network.fit(state, target, epochs=1, verbose=0)"
      ],
      "metadata": {
        "id": "ZiaUgw08aJhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "agent=Agent(enviroment,optimizer)\n",
        "agent = Agent(enviroment,optimizer)\n",
        "\n",
        "batch_size = 32\n",
        "num_of_episodes = 100\n",
        "timesteps_per_episode = 1000\n",
        "agent.q_network.summary()\n",
        "for e in range(0, num_of_episodes):\n",
        "    # Reset the enviroment\n",
        "    state = enviroment.reset()\n",
        "    state = np.reshape(state, [1, 1])\n",
        "    \n",
        "    # Initialize variables\n",
        "    reward = 0\n",
        "    terminated = False\n",
        "    \n",
        "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=\\\n",
        "[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    \n",
        "    for timestep in range(timesteps_per_episode):\n",
        "        # Run Action\n",
        "        action = agent.act(state)\n",
        "        \n",
        "        # Take action    \n",
        "        next_state, reward, terminated, info = enviroment.step(action) \n",
        "        next_state = np.reshape(next_state, [1, 1])\n",
        "        agent.store(state, action, reward, next_state, terminated)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if terminated:\n",
        "            agent.alighn_target_model()\n",
        "            break\n",
        "            \n",
        "        if len(agent.expirience_replay) > batch_size:\n",
        "            agent.retrain(batch_size)\n",
        "        \n",
        "        if timestep%10 == 0:\n",
        "            bar.update(timestep/10 + 1)\n",
        "    \n",
        "    bar.finish()\n",
        "    if (e + 1) % 10 == 0:\n",
        "        print(\"**********************************\")\n",
        "        print(\"Episode: {}\".format(e + 1))\n",
        "        enviroment.render()\n",
        "        print(\"**********************************\")"
      ],
      "metadata": {
        "id": "3mwpUnM6aJkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(0, num_of_episodes):\n",
        "    # Reset the enviroment\n",
        "    state = enviroment.reset()\n",
        "    state = np.reshape(state, [1, 1])\n",
        "    \n",
        "    # Initialize variables\n",
        "    reward = 0\n",
        "    terminated = False\n",
        "    \n",
        "    bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=\\\n",
        "[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    \n",
        "    for timestep in range(timesteps_per_episode):\n",
        "        # Run Action\n",
        "        action = agent.act(state)\n",
        "        \n",
        "        # Take action    \n",
        "        next_state, reward, terminated, info = enviroment.step(action) \n",
        "        next_state = np.reshape(next_state, [1, 1])\n",
        "        agent.store(state, action, reward, next_state, terminated)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if terminated:\n",
        "            agent.alighn_target_model()\n",
        "            break\n",
        "            \n",
        "        if len(agent.expirience_replay) > batch_size:\n",
        "            agent.retrain(batch_size)\n",
        "        \n",
        "        if timestep%10 == 0:\n",
        "            bar.update(timestep/10 + 1)\n",
        "    \n",
        "    bar.finish()\n",
        "    if (e + 1) % 10 == 0:\n",
        "        print(\"**********************************\")\n",
        "        print(\"Episode: {}\".format(e + 1))\n",
        "        enviroment.render()\n",
        "        print(\"**********************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "915K3Rb2aXtC",
        "outputId": "0cee3534-3969-4167-8d32-9e1a05284e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-5d8dc0029539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Reset the enviroment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menviroment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_of_episodes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple deep q learning with taxi v-03**"
      ],
      "metadata": {
        "id": "t8V3RZWPujv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "TYCfUEGPo7vt"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env=gym.make(\"Taxi-v3\")\n",
        "env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7exNtwRto701",
        "outputId": "9063866b-95e6-4aed-9b88-5574d348083f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeLimit<TaxiEnv<Taxi-v3>>>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state=env.reset()\n",
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf1dQTe6o73W",
        "outputId": "b585162d-e368-46bf-d00c-2cd0e900f557"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql6Glmiao75l",
        "outputId": "6f485048-421a-4925-bd7e-64ee99d65ab4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXU9UJJ0o778",
        "outputId": "c92730d5-2308-466e-df92-e75f246a973d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # set possible actions"
      ],
      "metadata": {
        "id": "Z_vrGkW9o7-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_states=env.observation_space.n\n",
        "n_action=env.action_space.n"
      ],
      "metadata": {
        "id": "cW1-xiE1o8Aq"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.env.s=150"
      ],
      "metadata": {
        "id": "cmTOS0VHo8im"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PWgVZugo8lG",
        "outputId": "a550822c-4fe5-4696-8318-8ab5061f312e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzpdqdI1o8no",
        "outputId": "8eba4342-bbdb-47bd-d346-1910eeea67e9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, -1, False, {'prob': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random behaviour"
      ],
      "metadata": {
        "id": "GAgdpg-Jo8qG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state=env.reset()\n",
        "counter= 0\n",
        "g= 0\n",
        "reward= None"
      ],
      "metadata": {
        "id": "f8kA27ku6lOA"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while reward != 20:\n",
        "  state, reward, done, info= env.step(env.action_space.sample())\n",
        "  counter += 1\n",
        "  g += reward\n"
      ],
      "metadata": {
        "id": "bLHOmJ326lQU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"solved in {} steps with total reward {}\".format(counter,g))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0U6tkeT6lSi",
        "outputId": "fdba149a-5599-4823-b880-a295809a1035"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solved in 6361 steps with total reward -25285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the changes in each step "
      ],
      "metadata": {
        "id": "VMURz4wa6lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q=np.zeros([n_states,n_action])\n",
        "q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp56wECZ6lXG",
        "outputId": "3d76a626-2d5f-42f6-d76c-ebc5de8cc86d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes=1      # which step we want to train\n",
        "G= 0            # new goal\n",
        "alpha=0.618     #learning rate (i can change it according to my comfort)"
      ],
      "metadata": {
        "id": "81R72xak_sj3"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(1,episodes+1):\n",
        "  done=False\n",
        "  G=0\n",
        "  reward=0\n",
        "  state=env.reset()\n",
        "  firststate=state\n",
        "  print(\"Initial state ={}\".format(state))\n",
        "  while reward != 20:\n",
        "    action=np.argmax(q[state])\n",
        "    state2,reward,done,info=env.step(action)\n",
        "    q[state,action]  += alpha * (reward + np.max(q[state2]) - q[state,action])\n",
        "    G += reward\n",
        "    state = state2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpTE3Crd_smM",
        "outputId": "b7f8b9e9-b04a-4ab2-d6ee-907adcb7e5d4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state =33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FinalState=state"
      ],
      "metadata": {
        "id": "tNr8lHpy_so2"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "firststate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHKjW3GE_srg",
        "outputId": "824eeb2b-07f7-4bb4-fbc5-f51963dce3fd"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FinalState"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hze7Tbaj_suO",
        "outputId": "e547cca0-0ee0-44e0-bf98-7eef458d5618"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtpGBnfmETbK",
        "outputId": "c229173e-acc1-4616-b6d5-e2dda697373e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       ...,\n",
              "       [-1.236   , -0.854076, -1.236   , -1.236   , -6.18    , -6.18    ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
              "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running multiple episodes for optimal policy"
      ],
      "metadata": {
        "id": "tmxi5ahPETld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes=3000\n",
        "rewardtracker = []"
      ],
      "metadata": {
        "id": "y5EKueaMETpm"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G= 0\n",
        "alpha= 0.618"
      ],
      "metadata": {
        "id": "wdSwEQm9ETtm"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(1,episodes+1):\n",
        "  done=False\n",
        "  G=0\n",
        "  reward=0\n",
        "  state=env.reset()\n",
        "  while done != True :\n",
        "    action=np.argmax(q[state])\n",
        "    state2,reward,done,info=env.step(action)\n",
        "    q[state,action]  += alpha * ((reward + (np.max(q[state2])) - q[state,action]))\n",
        "    G += reward\n",
        "    state = state2\n",
        "\n",
        "  if episode % 100 == 0:\n",
        "    print(\"Episode: {} Total Reward: {} \".format(episode,G))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQzYS8wiET4t",
        "outputId": "bca0cbbb-dc2d-42c1-a91a-86721f108e17"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100 Total Reward: -12 \n",
            "Episode: 200 Total Reward: -30 \n",
            "Episode: 300 Total Reward: 10 \n",
            "Episode: 400 Total Reward: 6 \n",
            "Episode: 500 Total Reward: 1 \n",
            "Episode: 600 Total Reward: 5 \n",
            "Episode: 700 Total Reward: 6 \n",
            "Episode: 800 Total Reward: 6 \n",
            "Episode: 900 Total Reward: 11 \n",
            "Episode: 1000 Total Reward: 4 \n",
            "Episode: 1100 Total Reward: 6 \n",
            "Episode: 1200 Total Reward: 7 \n",
            "Episode: 1300 Total Reward: 9 \n",
            "Episode: 1400 Total Reward: 6 \n",
            "Episode: 1500 Total Reward: 5 \n",
            "Episode: 1600 Total Reward: 11 \n",
            "Episode: 1700 Total Reward: 4 \n",
            "Episode: 1800 Total Reward: 12 \n",
            "Episode: 1900 Total Reward: 13 \n",
            "Episode: 2000 Total Reward: 9 \n",
            "Episode: 2100 Total Reward: 12 \n",
            "Episode: 2200 Total Reward: 9 \n",
            "Episode: 2300 Total Reward: 4 \n",
            "Episode: 2400 Total Reward: 8 \n",
            "Episode: 2500 Total Reward: 5 \n",
            "Episode: 2600 Total Reward: 7 \n",
            "Episode: 2700 Total Reward: 7 \n",
            "Episode: 2800 Total Reward: 7 \n",
            "Episode: 2900 Total Reward: 8 \n",
            "Episode: 3000 Total Reward: 12 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state=env.reset()\n",
        "done= None"
      ],
      "metadata": {
        "id": "IMAeyDHtIT91"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while done != True :\n",
        "    action=np.argmax(q[state])\n",
        "    state,reward,done,info=env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te7Tv1i-IUAe",
        "outputId": "0ac39eea-7e23-439b-da76-e33cf46dc3ba"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[42mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O_Ou4mjgIUCy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "la1dWdMBIUE5"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TVdzBsSOIUG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C-0qZpCXIUJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fahCZ8gvIUMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}