{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install important libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0.1: We can install libraries with the “pip” command by writing the following codes. It will install the library if it does not exist in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\compiler\\anaconda\\lib\\site-packages (1.20.1)\n",
      "Requirement already satisfied: pandas in d:\\compiler\\anaconda\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\compiler\\anaconda\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\compiler\\anaconda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\compiler\\anaconda\\lib\\site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\compiler\\anaconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: nltk in d:\\compiler\\anaconda\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in d:\\compiler\\anaconda\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in d:\\compiler\\anaconda\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in d:\\compiler\\anaconda\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in d:\\compiler\\anaconda\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: sklearn in d:\\compiler\\anaconda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\compiler\\anaconda\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\compiler\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\compiler\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\compiler\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\compiler\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: matplotlib in d:\\compiler\\anaconda\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\compiler\\anaconda\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\compiler\\anaconda\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\compiler\\anaconda\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\compiler\\anaconda\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\compiler\\anaconda\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\compiler\\anaconda\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in d:\\compiler\\anaconda\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orange pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\orange\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\orange\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary library’s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.1: We have to import some important libraries by writing the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.1: The dataset is stored in github and we are importing it by writing the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 353 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74682 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                              tweet\n",
       "0      Positive  im getting on borderlands and i will murder yo...\n",
       "1      Positive  I am coming to the borders and I will kill you...\n",
       "2      Positive  im getting on borderlands and i will kill you ...\n",
       "3      Positive  im coming on borderlands and i will murder you...\n",
       "4      Positive  im getting on borderlands 2 and i will murder ...\n",
       "...         ...                                                ...\n",
       "74677  Positive  Just realized that the Windows partition of my...\n",
       "74678  Positive  Just realized that my Mac window partition is ...\n",
       "74679  Positive  Just realized the windows partition of my Mac ...\n",
       "74680  Positive  Just realized between the windows partition of...\n",
       "74681  Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[74682 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imported in “tweets” which is an object of pandas. We can see a preview of the dataset by executing the following code\n",
    "\n",
    "The code will display a table containing five rows like the following. Because the function head() in pandas displays the first five rows of the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Positive</td>\n",
       "      <td>IN THE SO FICKING EXCELLENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Positive</td>\n",
       "      <td>The Proton-M launch vehicle that disappeared f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Positive</td>\n",
       "      <td>IM SO FUCKING IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Positive</td>\n",
       "      <td>IM AT SO GO FUCKING EXCITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Positive</td>\n",
       "      <td>IM SO WHAT EXCITED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                              tweet\n",
       "0    Positive  im getting on borderlands and i will murder yo...\n",
       "1    Positive  I am coming to the borders and I will kill you...\n",
       "2    Positive  im getting on borderlands and i will kill you ...\n",
       "3    Positive  im coming on borderlands and i will murder you...\n",
       "4    Positive  im getting on borderlands 2 and i will murder ...\n",
       "..        ...                                                ...\n",
       "595  Positive                        IN THE SO FICKING EXCELLENT\n",
       "596  Positive  The Proton-M launch vehicle that disappeared f...\n",
       "597  Positive                                   IM SO FUCKING IN\n",
       "598  Positive                        IM AT SO GO FUCKING EXCITED\n",
       "599  Positive                                 IM SO WHAT EXCITED\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1: Separate features and labels for further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = tweets[\"tweet\"]\n",
    "labels = tweets[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2: Clean individual tweets by removing hypterlinks, reply tags, special characters, extra spaces, single character and finally convert into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_features = []\n",
    "for sentence in range(0, len(features)):\n",
    "    processed_feature = re.sub(\"(@[A-Za-z0-9_]+)\", \"\",\n",
    "                               str(features[sentence]))\n",
    "    processed_feature = re.sub(\n",
    "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|' \\\n",
    "        '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', processed_feature)\n",
    " \n",
    "    processed_feature = re.sub(r\"'\", '', processed_feature)\n",
    " \n",
    "    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n",
    " \n",
    "    processed_feature = re.sub(r'\\s+[a-zA-Z1-9]\\s+', ' ', processed_feature)\n",
    " \n",
    "    processed_feature = re.sub(r'^[a-zA-Z1-9]\\s+', '', processed_feature) \n",
    " \n",
    "    processed_feature = re.sub(r'\\s+[a-zA-Z1-9]$', '', processed_feature) \n",
    " \n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature)\n",
    " \n",
    "    processed_feature = processed_feature.strip()\n",
    " \n",
    "    processed_feature = processed_feature.lower()\n",
    "    \n",
    "    processed_features.append(processed_feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.3:  Preview the cleaned dataset with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and will murder you all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am coming to the borders and will kill you all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and will kill you all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and will murder you all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands and will murder you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  im getting on borderlands and will murder you all\n",
       "1     am coming to the borders and will kill you all\n",
       "2    im getting on borderlands and will kill you all\n",
       "3   im coming on borderlands and will murder you all\n",
       "4  im getting on borderlands and will murder you ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(processed_features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.4: Tokenize each word then Remove StopWords and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmaTokens = []\n",
    "rawTokens = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Tokenizing\n",
    "for sentence in processed_features:\n",
    "    tempTokens = sentence.split()\n",
    "    rawTokens.append(tempTokens)\n",
    "    lemmatizedTokens = []\n",
    "    \n",
    "    # proceed lemmatization\n",
    "    for singleToken, tag in pos_tag(tempTokens):\n",
    "        # Preparing for morphological analysis\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "            \n",
    "        token = lemmatizer.lemmatize(singleToken, pos)\n",
    "        \n",
    "        if token not in stopwords.words(\"english\"):\n",
    "            if token.isdigit() == False:\n",
    "                lemmatizedTokens.append(token)\n",
    "        \n",
    "    # append the lemmatized tokens of sentence in the list\n",
    "    lemmaTokens.append(lemmatizedTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.5: Remove empty document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for tok in lemmaTokens:\n",
    "    if len(tok)==0:\n",
    "        lemmaTokens.pop(count)\n",
    "        rawTokens.pop(count)\n",
    "        labels.pop(count)\n",
    "        \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have cleaned token list of each tweets. Preview the dataset with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im</td>\n",
       "      <td>get</td>\n",
       "      <td>borderland</td>\n",
       "      <td>murder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>come</td>\n",
       "      <td>border</td>\n",
       "      <td>kill</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im</td>\n",
       "      <td>get</td>\n",
       "      <td>borderland</td>\n",
       "      <td>kill</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im</td>\n",
       "      <td>come</td>\n",
       "      <td>borderland</td>\n",
       "      <td>murder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im</td>\n",
       "      <td>get</td>\n",
       "      <td>borderland</td>\n",
       "      <td>murder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0       1           2       3     4     5     6     7     8     9    ...  \\\n",
       "0    im     get  borderland  murder  None  None  None  None  None  None  ...   \n",
       "1  come  border        kill    None  None  None  None  None  None  None  ...   \n",
       "2    im     get  borderland    kill  None  None  None  None  None  None  ...   \n",
       "3    im    come  borderland  murder  None  None  None  None  None  None  ...   \n",
       "4    im     get  borderland  murder  None  None  None  None  None  None  ...   \n",
       "\n",
       "    188   189   190   191   192   193   194   195   196   197  \n",
       "0  None  None  None  None  None  None  None  None  None  None  \n",
       "1  None  None  None  None  None  None  None  None  None  None  \n",
       "2  None  None  None  None  None  None  None  None  None  None  \n",
       "3  None  None  None  None  None  None  None  None  None  None  \n",
       "4  None  None  None  None  None  None  None  None  None  None  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lemmaTokens).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.6: Convert the Text data into numerical form by Calculating TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.6.1: Calculate TF. In order to calculate TFIDF we will need to calculate TF then IDF and then finally TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "words = set()\n",
    "for documentTokens in lemmaTokens:\n",
    "    for token in documentTokens:\n",
    "        words.add(token)\n",
    "uniqueWords = list(words)\n",
    "documents = []\n",
    "for i in range(0,len(lemmaTokens)):\n",
    "    documents.append(dict.fromkeys(uniqueWords,0))\n",
    "    \n",
    "count = 0\n",
    "for documentTokens in lemmaTokens:\n",
    "    for token in documentTokens:\n",
    "        documents[count][token]+=1\n",
    "    count+=1\n",
    "\n",
    "def calculateTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        if bowCount == 0:\n",
    "            print(wordDict)\n",
    "            print(bow)\n",
    "        try:\n",
    "            tfDict[word] = count/float(bowCount)\n",
    "        except ZeroDivisionError:\n",
    "            print(\"error\")\n",
    "    return tfDict\n",
    " \n",
    "TF = []\n",
    "count = 0\n",
    "for document in documents:\n",
    "    TF.append(calculateTF(document,lemmaTokens[count]))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.6.2: Calculate IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "IDFs = calculateIDF(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.6.3: Calculate TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    " \n",
    "def calculateTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    " \n",
    "TFIDF = []\n",
    "count = 0\n",
    "for document in TF:\n",
    "    TFIDF.append(calculateTFIDF(document,IDFs))\n",
    "    count += 1\n",
    "\n",
    "TFIDF = pd.DataFrame(TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully converted our text data into numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split Train and Test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train different Machine Learning Algorithms, we have to split our dataset into train and test data. The following code will split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(TFIDF, labels, test_size=0.3, random_state = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train Machine Learning Algorithms with Training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5.1: Training with Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decisionTree = DecisionTreeClassifier()\n",
    "decisionTree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5.2: Training with Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naiveBayes = MultinomialNB()\n",
    "naiveBayes.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5.3: Training with KNN(K- Nearest Neighbors) classifier ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5.4: Training with Support Vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Training Ensemble Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6.1: Training with Voting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn. ensemble import VotingClassifier\n",
    "votingClf = VotingClassifier(estimators=[('decisionTree',decisionTree),\n",
    "('naiveBayes',naiveBayes),('knn',knn),('svc',svc)],voting='hard')\n",
    "votingClf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6.2: Training with Bagging Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier(n_estimators=4)\n",
    "bagging.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6.3: Training with AdaBoost Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaBoost = AdaBoostClassifier(n_estimators=4)\n",
    "adaBoost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6.4: Training with RandomForest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomForest = RandomForestClassifier(n_estimators=4)\n",
    "randomForest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Predict the labels of Test data using Trained Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.1: Predicting with Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtPreds = decisionTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.2: Predicting with Naïve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nbPreds = naiveBayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.3: Predicting with KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knnPreds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.4: Predicting with Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svmPreds = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.5: Predicting with Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "votingPreds = votingClf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.6: Predicting with Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bagPreds = bagging.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.7: Predicting with AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adaBoostPreds = adaBoost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7.8: Predicting with RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "randomForestPreds = randomForest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Finding Confusion Matrix, Classification Report, Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.1: For Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,dtPreds))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,dtPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, dtPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.2: For Naïve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,nbPreds))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,nbPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, nbPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.3: For KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,knnPreds))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,knnPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, knnPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.4: For Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,svmPreds))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,svmPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, svmPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.5: For Voting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,votingPreds))\n",
    "print('\\nClassification Report:\\n\\n',\n",
    "\tclassification_report(y_test,votingPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, votingPreds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.5: For Bagging Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,bagPreds))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,bagPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, bagPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.6: For AdaBoost Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,adaBoostPreds))\n",
    "print('\\nClassification Report:\\n\\n',\n",
    "classification_report(y_test,adaBoostPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, adaBoostPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8.7: For RandomForest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,randomForestPreds))\n",
    "print('\\nClassification Report:\\n\\n',\n",
    "classification_report(y_test,randomForestPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, randomForestPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Implementing proposed or TieWeakVoting Ensemble Learning algorithm which is updated version of the Voting Ensemble Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9.1: Implementation of the TieWeakVoting Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "import time\n",
    "\n",
    "\n",
    "class TieWeakVoting:\n",
    "    def __init__(self,estimators,allPreds=None):\n",
    "        self.estimators = estimators\n",
    "        self.allPreds = allPreds\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        print(\"Finding weak label on traing data\")\n",
    "        \n",
    "        uniqueEntry = self.getUniqueEntry(y_train)\n",
    "        print(uniqueEntry)\n",
    "        entryCount = self.getEntryCount(uniqueEntry,y_train)\n",
    "        print(entryCount)\n",
    "        self.weak = self.findWeakLabel(entryCount)\n",
    "        print(\"Weak Label Found: \"+self.weak)\n",
    "        \n",
    "        for clf in self.estimators:\n",
    "            startTime = time.time()\n",
    "            print(clf[0] + \" Fitting...\")\n",
    "            clf[1].fit(X_train,y_train)\n",
    "            print(clf[0] +\" Fitted in \"+str((time.time()- startTime)))\n",
    "        print(\"Fitting Successful\")\n",
    "       \n",
    "    \n",
    "    def getEntryCount(self,uniqueEntry,y_train):\n",
    "        entryCount = dict()\n",
    "        for name in uniqueEntry:\n",
    "            entryCount[name] = 0\n",
    "\n",
    "        for row in y_train:\n",
    "            for entry in uniqueEntry:\n",
    "                if row==entry:\n",
    "                    entryCount[entry]  = entryCount[entry] + 1\n",
    "        \n",
    "        return entryCount\n",
    "    \n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        self.allPreds = pd.DataFrame()\n",
    "        \n",
    "        #Predictin with estimators\n",
    "        for clf in self.estimators:\n",
    "            \n",
    "            start_time = time.time() \n",
    "            print(clf[0] +\" Predicting...\")\n",
    "            preds = clf[1].predict(X_test)\n",
    "            predsDF = panda.DataFrame(preds)\n",
    "            self.allPreds[clf[0]] = predsDF[0]\n",
    "            print(clf[0] +\" Predicted in \"+str((time.time()- start_time)))\n",
    "        \n",
    "        \n",
    "        print(\"Applying Ensemble Learning Algorihtm on Predicted Data.\")\n",
    "        \n",
    "        \n",
    "        return self.getPreds()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getPreds(self):\n",
    "        newPreds = list()\n",
    "\n",
    "        for index, row in self.allPreds.iterrows():\n",
    "            totalItemDict = dict()\n",
    "            tempSet = set(row)\n",
    "\n",
    "            for item in tempSet:\n",
    "                totalItemDict[item] = 0\n",
    "\n",
    "            for item in list(row):\n",
    "                totalItemDict[item] = totalItemDict[item] + 1\n",
    "\n",
    "            newPreds.append(self.chooseLabel(totalItemDict,self.weak))\n",
    "        print(\"Prediction Successcul\")\n",
    "        return newPreds\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getUniqueEntry(self,y_train):\n",
    "        return list(y_train.drop_duplicates())\n",
    "    \n",
    "    \n",
    "    def findWeakLabel(self,entryCount):\n",
    "        entries = list(entryCount.items())\n",
    "\n",
    "        label = entries[0][0]\n",
    "        weak = entries[0][1]\n",
    "\n",
    "        for count in entries:\n",
    "\n",
    "            if weak>count[1]:\n",
    "                weak = count[1]\n",
    "                label = count[0] \n",
    "        return label\n",
    "    \n",
    "    \n",
    "    def chooseLabel(self,entryCount,weak):\n",
    "        high = 0\n",
    "        highLabel = ''\n",
    "        low = 0\n",
    "        lowLabel = ''\n",
    "\n",
    "        entries = list(entryCount.items())\n",
    "        finalLabel = ''\n",
    "        if len(entries)>1:\n",
    "            evenCount = 0\n",
    "            for count in entries:\n",
    "                if count[1]<high:\n",
    "                    low = count[1]\n",
    "                elif count[1]>high:\n",
    "                    weakMark = False\n",
    "                    high = count[1]\n",
    "                    highLabel = count[0]\n",
    "                else:\n",
    "                    weakMark = True\n",
    "                    low = count[1]\n",
    "                    lowLabel = count[0]\n",
    "\n",
    "\n",
    "            if low<high and not weakMark:\n",
    "                return highLabel\n",
    "            else:\n",
    "                return weak\n",
    "        else:\n",
    "            return entries[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9.2: Training with TieWeakVoting Algorithm with Traning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tieWeakVoting = TieWeakVoting(estimators=[('decisionTree',decisionTree),\n",
    "('naiveBayes',naiveBayes),('knn',knn),('svc',svc)])\n",
    "tieWeakVoting.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9.3: Predicting with TieWeakVoting Algorithm with Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twvPreds = tieWeakVoting.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the predictions from TieWeakVoting Algorithm in ‘twvPred’ variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9.4: Finding Confusion Matrix, Classification Report, Accuracy Score of TieWeakVoting Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,twvPreds))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,twvPreds))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, twvPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf  = RandomForestClassifier()\n",
    "Sfolds = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "sc_Srf=cross_val_score(rf,TFIDF,labels,cv=Sfolds)*100\n",
    "print('Random Forest--> Accuracy for all iteration ', sc_Srf)\n",
    "ME_Srf = sc_Srf.mean()\n",
    "print('Random Forest-->Mean Accuracy %.2f' %ME_Srf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "OvR = OneVsRestClassifier(RandomForestClassifier())\n",
    "OvR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf  = RandomForestClassifier(n_estimators=50)\n",
    "Sfolds = StratifiedKFold(n_splits=4,shuffle=True)\n",
    "sc_Srf=cross_val_score(rf,TFIDF,labels,cv=20)*100\n",
    "print('Random Forest--> Accuracy for all iteration ', sc_Srf)\n",
    "ME_Srf = sc_Srf.mean()\n",
    "print('Random Forest-->Mean Accuracy %.2f' %ME_Srf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf  = RandomForestClassifier(n_estimators=50)\n",
    "Sfolds = StratifiedKFold(n_splits=246,shuffle=True)\n",
    "sc_Srf=cross_val_score(rf,TFIDF,labels,cv=20)*100\n",
    "print('Random Forest--> Accuracy for all iteration ', sc_Srf)\n",
    "ME_Srf = sc_Srf.mean()\n",
    "print('Random Forest-->Mean Accuracy %.2f' %ME_Srf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\orange pc\\Downloads\\new.csv')  #new.csv = TFIDF data \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df\n",
    "y=df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0) \n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "correct_labels = sum(y == labels)\n",
    "print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array=df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "model = 1\n",
    "for train, test in kfold.split(np_array):\n",
    " print('Model #%d:' % model)\n",
    " print('train: %s, test: %s' % (train, test))\n",
    "model = model+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df.drop('labels',axis=1)\n",
    "y1=df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.2, random_state=42,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt= DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "y_pred=dt.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test,y_pred))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test,y_pred))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model = BaggingClassifier(\n",
    "base_estimator=DecisionTreeClassifier(), \n",
    "n_estimators=100, \n",
    "max_samples=0.8, \n",
    "bootstrap=True,\n",
    "oob_score=True,\n",
    "random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model1 = BaggingClassifier(\n",
    "base_estimator=svm.SVC(), \n",
    "n_estimators=100, \n",
    "max_samples=0.8, \n",
    "bootstrap=True,\n",
    "oob_score=True,\n",
    "random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_model1.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustered_Sampling(df, n_per_cluster, num_select_clusters):\n",
    "    N = len(df)\n",
    "    K = int(N/n_per_cluster)\n",
    "    data = None\n",
    "    for k in range(K):\n",
    "        sample_k = df.sample(n_per_cluster)\n",
    "        sample_k[\"cluster\"] = np.repeat(k,len(sample_k))\n",
    "        df = df.drop(index = sample_k.index)\n",
    "        data = pd.concat([data,sample_k],axis = 0)\n",
    "\n",
    "    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)\n",
    "    samples = data[data.cluster.isin(random_chosen_clusters)]\n",
    "    return(samples)\n",
    "\n",
    "sample = clustered_Sampling(df = df, n_per_cluster = 2806, num_select_clusters = 5)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_sampling(df, step):\n",
    " \n",
    "    indexes = np.arange(0, len(df), step=step)\n",
    "    systematic_sample = df.iloc[indexes]\n",
    "    return systematic_sample\n",
    " \n",
    "systematic_sample = systematic_sampling(df,5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=sample.drop('labels',axis=1)\n",
    "y2=sample['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x2, y2, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt= DecisionTreeClassifier()\n",
    "dt.fit(x_train1,y_train1)\n",
    "y_pred10=dt.predict(x_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print('\\nConfusion Matrix:\\n\\n',confusion_matrix(y_test1,y_pred10))\n",
    "print('\\nClassification Report:\\n\\n',classification_report(y_test1,y_pred10))\n",
    "print('\\nAccuracy Score: ',accuracy_score(y_test1, y_pred10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
