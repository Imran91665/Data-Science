{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Crossentropy method\nThis notebook will teach you to solve reinforcement learning problems with crossentropy method. We'll follow-up by scaling everything up and using . neural network policy.","metadata":{}},{"cell_type":"code","source":"# In Google Colab, uncomment this:\n!wget https://bit.ly/2FMJP5K -O setup.py && bash setup.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XVFB will be launched if you run on a server\nimport os\nif type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n    !bash ../xvfb start\n    os.environ['DISPLAY'] = ':1'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport pandas as pd\n\nenv = gym.make(\"Taxi-v3\")\nenv.reset()\nenv.render()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:46:52.673828Z","iopub.execute_input":"2022-07-27T10:46:52.674361Z","iopub.status.idle":"2022-07-27T10:46:53.397534Z","shell.execute_reply.started":"2022-07-27T10:46:52.674284Z","shell.execute_reply":"2022-07-27T10:46:53.396708Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"n_states = env.observation_space.n\nn_actions = env.action_space.n\n\nprint(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:47:05.602419Z","iopub.execute_input":"2022-07-27T10:47:05.602727Z","iopub.status.idle":"2022-07-27T10:47:05.608318Z","shell.execute_reply.started":"2022-07-27T10:47:05.602676Z","shell.execute_reply":"2022-07-27T10:47:05.607699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"policy = np.ones((n_states, n_actions))/(n_actions)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:47:52.012400Z","iopub.execute_input":"2022-07-27T10:47:52.012977Z","iopub.status.idle":"2022-07-27T10:47:52.016863Z","shell.execute_reply.started":"2022-07-27T10:47:52.012924Z","shell.execute_reply":"2022-07-27T10:47:52.016088Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"assert type(policy) in (np.ndarray, np.matrix)\nassert np.allclose(policy, 1./n_actions)\nassert np.allclose(np.sum(policy, axis=1), 1)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:48:04.892233Z","iopub.execute_input":"2022-07-27T10:48:04.892800Z","iopub.status.idle":"2022-07-27T10:48:04.897455Z","shell.execute_reply.started":"2022-07-27T10:48:04.892735Z","shell.execute_reply":"2022-07-27T10:48:04.896900Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def generate_session(policy, t_max=10**4):\n    \"\"\"\n    Play game until end or for t_max ticks.\n    :param policy: an array of shape [n_states,n_actions] with action probabilities\n    :returns: list of states, list of actions and sum of rewards\n    \"\"\"\n    states, actions = [], []\n    total_reward = 0.\n\n    s = env.reset()\n\n    for t in range(t_max):\n\n        #a = <sample action from policy(hint: use np.random.choice) >\n        a = np.random.choice(n_actions, 1, p=policy[s])[0]\n\n        new_s, r, done, info = env.step(a)\n\n        # Record state, action and add up reward to states,actions and total_reward accordingly.\n        states.append(s)\n        actions.append(a)\n        total_reward += r\n\n        s = new_s\n        if done:\n            break\n    return states, actions, total_reward","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:48:29.999810Z","iopub.execute_input":"2022-07-27T10:48:30.000374Z","iopub.status.idle":"2022-07-27T10:48:30.006455Z","shell.execute_reply.started":"2022-07-27T10:48:30.000327Z","shell.execute_reply":"2022-07-27T10:48:30.005877Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"s, a, r = generate_session(policy)\nassert type(s) == type(a) == list\nassert len(s) == len(a)\nassert type(r) in [float, np.float]","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:48:37.292508Z","iopub.execute_input":"2022-07-27T10:48:37.293086Z","iopub.status.idle":"2022-07-27T10:48:37.312385Z","shell.execute_reply.started":"2022-07-27T10:48:37.293035Z","shell.execute_reply":"2022-07-27T10:48:37.311726Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# let's see the initial reward distribution\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsample_rewards = [generate_session(policy, t_max=1000)[-1] for _ in range(200)]\n\nplt.hist(sample_rewards, bins=20)\nplt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\nplt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:48:42.740886Z","iopub.execute_input":"2022-07-27T10:48:42.741470Z","iopub.status.idle":"2022-07-27T10:48:44.806427Z","shell.execute_reply.started":"2022-07-27T10:48:42.741414Z","shell.execute_reply":"2022-07-27T10:48:44.805534Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Crossentropy method steps","metadata":{}},{"cell_type":"code","source":"def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n    \"\"\"\n    Select states and actions from games that have rewards >= percentile\n    :param states_batch: list of lists of states, states_batch[session_i][t]\n    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n    :param rewards_batch: list of rewards, rewards_batch[session_i]\n\n    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n\n    Please return elite states and actions in their original order \n    [i.e. sorted by session number and timestep within session]\n\n    If you are confused, see examples below. Please don't assume that states are integers\n    (they will become different later).\n    \"\"\"\n\n#     reward_threshold = <Compute minimum reward for elite sessions. Hint: use np.percentile >\n    reward_threshold = np.percentile(rewards_batch, percentile)\n    elite_states = []\n    elite_actions = []\n    for i in range(len(rewards_batch)):\n        if rewards_batch[i] >= reward_threshold:\n            elite_states = elite_states + states_batch[i]\n            elite_actions = elite_actions +actions_batch[i]\n    return elite_states, elite_actions","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:49:21.474909Z","iopub.execute_input":"2022-07-27T10:49:21.475522Z","iopub.status.idle":"2022-07-27T10:49:21.483141Z","shell.execute_reply.started":"2022-07-27T10:49:21.475458Z","shell.execute_reply":"2022-07-27T10:49:21.482170Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"states_batch = [\n    [1, 2, 3],     # game1\n    [4, 2, 0, 2],  # game2\n    [3, 1],        # game3\n]\n\nactions_batch = [\n    [0, 2, 4],     # game1\n    [3, 2, 0, 1],  # game2\n    [3, 3],        # game3\n]\nrewards_batch = [\n    3,  # game1\n    4,  # game2\n    5,  # game3\n]\n\ntest_result_0 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=0)\ntest_result_40 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=30)\ntest_result_90 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=90)\ntest_result_100 = select_elites(\n    states_batch, actions_batch, rewards_batch, percentile=100)\n\nassert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n    \"For percentile 0 you should return all states and actions in chronological order\"\nassert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n    np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]),\\\n    \"For percentile 30 you should only select states/actions from two first\"\nassert np.all(test_result_90[0] == [3, 1]) and \\\n    np.all(test_result_90[1] == [3, 3]),\\\n    \"For percentile 90 you should only select states/actions from one game\"\nassert np.all(test_result_100[0] == [3, 1]) and\\\n    np.all(test_result_100[1] == [3, 3]),\\\n    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\nprint(\"Ok!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:49:28.307509Z","iopub.execute_input":"2022-07-27T10:49:28.308028Z","iopub.status.idle":"2022-07-27T10:49:28.321629Z","shell.execute_reply.started":"2022-07-27T10:49:28.307974Z","shell.execute_reply":"2022-07-27T10:49:28.320937Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def update_policy(elite_states, elite_actions):\n    \"\"\"\n    Given old policy and a list of elite states/actions from select_elites,\n    return new updated policy where each action probability is proportional to\n\n    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n\n    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n    In case you never visited a state, set probabilities for all actions to 1./n_actions\n\n    :param elite_states: 1D list of states from elite sessions\n    :param elite_actions: 1D list of actions from elite sessions\n\n    \"\"\"\n\n    new_policy = np.zeros([n_states, n_actions])\n\n#     <Your code here: update probabilities for actions given elite states & actions >\n    # Don't forget to set 1/n_actions for all actions in unvisited states.\n    for e_i in range(len(elite_states)):\n        # Add 1 for every elite state and action\n        new_policy[elite_states[e_i], elite_actions[e_i]] += 1\n    # check for each state\n    for s_i in range(n_states):\n        total_actions = sum(new_policy[s_i])\n        if total_actions != 0:\n            new_policy[s_i] = new_policy[s_i]/total_actions\n        else:\n            new_policy[s_i] = np.ones(n_actions)/n_actions\n    return new_policy","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:49:54.475572Z","iopub.execute_input":"2022-07-27T10:49:54.476125Z","iopub.status.idle":"2022-07-27T10:49:54.482380Z","shell.execute_reply.started":"2022-07-27T10:49:54.476076Z","shell.execute_reply":"2022-07-27T10:49:54.481692Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\nelite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n\nnew_policy = update_policy(elite_states, elite_actions)\n\nassert np.isfinite(new_policy).all(\n), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\nassert np.all(\n    new_policy >= 0), \"Your new policy can't have negative action probabilities\"\nassert np.allclose(new_policy.sum(\n    axis=-1), 1), \"Your new policy should be a valid probability distribution over actions\"\nreference_answer = np.array([\n    [1.,  0.,  0.,  0.,  0.],\n    [0.5,  0.,  0.,  0.5,  0.],\n    [0.,  0.33333333,  0.66666667,  0.,  0.],\n    [0.,  0.,  0.,  0.5,  0.5]])\nassert np.allclose(new_policy[:4, :5], reference_answer)\nprint(\"Ok!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:50:00.754059Z","iopub.execute_input":"2022-07-27T10:50:00.754917Z","iopub.status.idle":"2022-07-27T10:50:00.772355Z","shell.execute_reply.started":"2022-07-27T10:50:00.754843Z","shell.execute_reply":"2022-07-27T10:50:00.771300Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Training loop\nGenerate sessions, select N best and fit to those.","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n\ndef show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n    \"\"\"\n    A convenience function that displays training progress. \n    No cool math here, just charts.\n    \"\"\"\n\n    mean_reward = np.mean(rewards_batch)\n    threshold = np.percentile(rewards_batch, percentile)\n    log.append([mean_reward, threshold])\n\n    clear_output(True)\n    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n    plt.figure(figsize=[8, 4])\n    plt.subplot(1, 2, 1)\n    plt.plot(list(zip(*log))[0], label='Mean rewards')\n    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n    plt.legend()\n    plt.grid()\n\n    plt.subplot(1, 2, 2)\n    plt.hist(rewards_batch, range=reward_range)\n    plt.vlines([np.percentile(rewards_batch, percentile)],\n               [0], [100], label=\"percentile\", color='red')\n    plt.legend()\n    plt.grid()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:50:20.689940Z","iopub.execute_input":"2022-07-27T10:50:20.690390Z","iopub.status.idle":"2022-07-27T10:50:20.698343Z","shell.execute_reply.started":"2022-07-27T10:50:20.690348Z","shell.execute_reply":"2022-07-27T10:50:20.697659Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# reset policy just in case\npolicy = np.ones([n_states, n_actions]) / n_actions","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:50:29.975289Z","iopub.execute_input":"2022-07-27T10:50:29.975752Z","iopub.status.idle":"2022-07-27T10:50:29.979253Z","shell.execute_reply.started":"2022-07-27T10:50:29.975672Z","shell.execute_reply":"2022-07-27T10:50:29.978653Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"n_sessions = 250  # sample this many sessions\npercentile = 50  # take this percent of session with highest rewards\nlearning_rate = 0.5  # add this thing to all counts for stability\n\nlog = []\n\nfor i in range(100):\n\n#     %time sessions = [ < generate a list of n_sessions new sessions > ]\n    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n    \n    states_batch, actions_batch, rewards_batch = zip(*sessions)\n\n#     elite_states, elite_actions = <select elite states/actions >\n    elite_states, elite_actions = select_elites(states_batch=states_batch, actions_batch=actions_batch, rewards_batch=rewards_batch, percentile=percentile)\n\n#     new_policy = <compute new policy >\n    new_policy = update_policy(elite_states=elite_states, elite_actions=elite_actions)\n    \n    policy = learning_rate*new_policy + (1-learning_rate)*policy\n\n    # display results on chart\n    show_progress(rewards_batch, log, percentile)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T10:50:37.331212Z","iopub.execute_input":"2022-07-27T10:50:37.331624Z","iopub.status.idle":"2022-07-27T10:52:03.103304Z","shell.execute_reply.started":"2022-07-27T10:50:37.331586Z","shell.execute_reply":"2022-07-27T10:52:03.102223Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}